{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for k, v in train_data.items():\n",
    "    v = k.split()\n",
    "    for word in v:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_idx(word):\n",
    "    return vocab.index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(idx):\n",
    "    return vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OneHotEncode(word):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[word_to_idx(word)] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_max_length = 10\n",
    "def createInputs(inputs):\n",
    "    v = []\n",
    "    w = []\n",
    "    for word in inputs.split():\n",
    "        v.append(OneHotEncode(word))\n",
    "    for i in range(sentence_max_length - len(v)):\n",
    "        v.append(np.zeros(vocab_size))\n",
    "    w.append(v)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.])]]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = createInputs('i am happy') # Result can be different\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 18])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.tensor(w[0])\n",
    "v = v.to(torch.float32)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('data/train_data.csv')\n",
    "dict = train_df.set_index('X').T.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': True,\n",
       " 'bad': False,\n",
       " 'happy': True,\n",
       " 'sad': False,\n",
       " 'not good': False,\n",
       " 'not bad': True,\n",
       " 'not happy': False,\n",
       " 'not sad': True,\n",
       " 'very good': True,\n",
       " 'very bad': False,\n",
       " 'very happy': True,\n",
       " 'very sad': False,\n",
       " 'i am happy': True,\n",
       " 'this is good': True,\n",
       " 'i am bad': False,\n",
       " 'this is bad': False,\n",
       " 'i am sad': False,\n",
       " 'this is sad': False,\n",
       " 'i am not happy': False,\n",
       " 'this is not good': False,\n",
       " 'i am not bad': True,\n",
       " 'this is not sad': True,\n",
       " 'i am very happy': True,\n",
       " 'this is very good': True,\n",
       " 'i am very bad': False,\n",
       " 'this is very sad': False,\n",
       " 'this is very happy': True,\n",
       " 'i am good not bad': True,\n",
       " 'this is good not bad': True,\n",
       " 'i am bad not good': False,\n",
       " 'i am good and happy': True,\n",
       " 'this is not good and not happy': False,\n",
       " 'i am not at all good': False,\n",
       " 'i am not at all bad': True,\n",
       " 'i am not at all happy': False,\n",
       " 'this is not at all sad': True,\n",
       " 'this is not at all happy': False,\n",
       " 'i am good right now': True,\n",
       " 'i am bad right now': False,\n",
       " 'this is bad right now': False,\n",
       " 'i am sad right now': False,\n",
       " 'i was good earlier': True,\n",
       " 'i was happy earlier': True,\n",
       " 'i was bad earlier': False,\n",
       " 'i was sad earlier': False,\n",
       " 'i am very bad right now': False,\n",
       " 'this is very good right now': True,\n",
       " 'this is very sad right now': False,\n",
       " 'this was bad earlier': False,\n",
       " 'this was very good earlier': True,\n",
       " 'this was very bad earlier': False,\n",
       " 'this was very happy earlier': True,\n",
       " 'this was very sad earlier': False,\n",
       " 'i was good and not bad earlier': True,\n",
       " 'i was not good and not happy earlier': False,\n",
       " 'i am not at all bad or sad right now': True,\n",
       " 'i am not at all good or happy right now': False,\n",
       " 'this was not happy and not good earlier': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "dct = {}\n",
    "for k, v in dict.items():\n",
    "    label = str(v).translate(str.maketrans('', '', string.punctuation))\n",
    "    if label == 'True':\n",
    "        b = True\n",
    "    if label == 'False':\n",
    "        b = False\n",
    "    dct[k] = b\n",
    "dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(matrix):\n",
    "    mean = torch.mean(matrix, dim = 1, keepdim = True)\n",
    "    std = torch.std(matrix, dim = 1, keepdim = True)\n",
    "    z_score_vector = (matrix-mean)/std\n",
    "    return z_score_vector\n",
    "\n",
    "def self_attention(mask, Q, K, V):\n",
    "    att = (Q @ K.T + mask)/np.sqrt(Q.shape[0])\n",
    "    return (torch.softmax(att, dim = 0) @ V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer():\n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        # That is the number of heads\n",
    "        self.n_heads = 2\n",
    "        # Here we can define matrices in pytorch with require_grad = True\n",
    "        # Initialize matrices for self-attention layer\n",
    "        self.Wq = torch.empty(0, input_size)\n",
    "        self.Wk = torch.empty(0, input_size)\n",
    "        self.Wv = torch.empty(0, input_size)\n",
    "        for i in range(self.n_heads):\n",
    "            Wq = torch.rand(hidden_size, input_size)/1000\n",
    "            self.Wq = torch.cat((self.Wq, Wq), dim = 0)\n",
    "            Wk = torch.rand(hidden_size, input_size)/1000\n",
    "            self.Wk = torch.cat((self.Wk, Wk), dim = 0)\n",
    "            Wv = torch.rand(hidden_size, input_size)/1000\n",
    "            self.Wv = torch.cat((self.Wv, Wv), dim = 0)\n",
    "        \n",
    "        self.Wq.requires_grad = True\n",
    "        self.Wk.requires_grad = True\n",
    "        self.Wv.requires_grad = True\n",
    "\n",
    "        # Initialize the matrix Wo for the projection of the multi head layer output\n",
    "        self.Wo = torch.rand(hidden_size, self.n_heads * hidden_size)/1000\n",
    "        self.Wo.requires_grad = True\n",
    "        # Create the mask matrix with upper part is -infinity\n",
    "        # and other entries are 0\n",
    "        # First we create a matrix filled with - infinity at all entries\n",
    "        matrix = torch.full((self.n_heads*hidden_size, self.n_heads * hidden_size), -float('inf'))\n",
    "        # triu = triangular upper part, we want to fill in upper part with - infinity\n",
    "        matrix = torch.triu(matrix)\n",
    "        # Replace the diagonal by 0\n",
    "        matrix = matrix.fill_diagonal_(0)\n",
    "        self.mask = matrix\n",
    "        # That is for the positional encoding\n",
    "        self.n = 10000\n",
    "\n",
    "\n",
    "        # Dimension matching layer for the layer norm\n",
    "        # This layer norm is after self-attention layer\n",
    "        self.Wnorm_self = torch.rand(hidden_size, input_size)/1000\n",
    "        self.Wnorm_self.requires_grad = True\n",
    "        self.bnorm_self = torch.zeros((hidden_size,1), requires_grad=True)\n",
    "        self.gamma_self = torch.rand((sentence_max_length,hidden_size))/1000\n",
    "        self.gamma_self.requires_grad = True\n",
    "        self.beta_self = torch.zeros((hidden_size,1), requires_grad=True)\n",
    "\n",
    "        # Initialize matrices for feed-forward layer\n",
    "        self.Wf = torch.rand(output_size, hidden_size)/1000\n",
    "        self.Wf.requires_grad = True\n",
    "        self.bf = torch.zeros((output_size,1), requires_grad=True)\n",
    "\n",
    "        # Initialize parameters for layer normalization after feedforward layer\n",
    "        self.Wnorm_feed = torch.rand(output_size, hidden_size)/1000\n",
    "        self.Wnorm_feed.requires_grad = True\n",
    "        self.bnorm_feed = torch.rand((output_size,1), requires_grad=True)\n",
    "        self.gamma_feed = torch.rand((hidden_size,1))/1000\n",
    "        self.gamma_feed.requires_grad = True\n",
    "        self.beta_feed = torch.zeros((output_size,1), requires_grad=True)    \n",
    "    \n",
    "    # Positional encoding for the whole sentence\n",
    "    def positional_encoding(self, inputs):\n",
    "        pos_encoding = torch.empty(inputs.shape[0],0)\n",
    "        iter = inputs.shape[1]\n",
    "        for k in range(iter):\n",
    "            d = inputs.shape[0]\n",
    "            pos = torch.zeros((d,1))\n",
    "            for i in range(d):\n",
    "                if i % 2 == 0:\n",
    "                    pos[i] = np.sin(k/(self.n**(i/d)))\n",
    "                if i % 2 == 1:\n",
    "                    pos[i] = np.cos(k/(self.n**((i-1)/d)))\n",
    "            v_pos = inputs[:, k].reshape(-1,1) + pos\n",
    "            pos_encoding = torch.cat((pos_encoding, v_pos), dim = 1)\n",
    "        return pos_encoding\n",
    "    \n",
    "    # Multi-head attention\n",
    "    def multi_head_attention(self, inputs):\n",
    "        q = self.Wq @ inputs\n",
    "        k = self.Wk @ inputs\n",
    "        v = self.Wv @ inputs\n",
    "        attention = self_attention(self.mask, q, k, v)\n",
    "        output = self.Wo @ attention\n",
    "        return output\n",
    "        \n",
    "    # Layer norm after self-attention layer\n",
    "    def layer_norm_self_attention(self, previous_layer_outputs, inputs):\n",
    "        inputs_matching = self.Wnorm_self @ inputs + self.bnorm_self\n",
    "        vector = previous_layer_outputs + inputs_matching\n",
    "        z_score_vector = z_score(vector)\n",
    "        next_inputs = z_score_vector @ self.gamma_self + self.beta_self\n",
    "        return next_inputs\n",
    "\n",
    "    # Feed forward after layer norm\n",
    "    def feed_forward(self, inputs):\n",
    "        output = self.Wf @ inputs + self.bf\n",
    "        output = torch.relu(output)\n",
    "        return output\n",
    "        \n",
    "    # Layer norm after feed_forward\n",
    "    def layer_norm_feed_forward(self, previous_layer_outputs, inputs):\n",
    "        intputs_matching = self.Wnorm_feed @ inputs + self.bnorm_feed\n",
    "        vector = previous_layer_outputs + intputs_matching\n",
    "        z_score_vector = z_score(vector)\n",
    "        next_inputs = z_score_vector @ self.gamma_feed + self.beta_feed\n",
    "        return next_inputs\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Inputs is a whole sentence at a time\n",
    "        # Inputs will be a list with only one element\n",
    "        # And this element is the matrix representation of the sentence\n",
    "        # The matrix consists of row vectors, each row is a word embedding\n",
    "        torch_inputs = torch.tensor(inputs[0])\n",
    "        torch_inputs = torch_inputs.T\n",
    "        torch_inputs = torch_inputs.to(torch.float32)\n",
    "        pos_inputs = self.positional_encoding(torch_inputs)\n",
    "        output_self_attention = self.multi_head_attention(pos_inputs)\n",
    "        output_layer_norm_self = self.layer_norm_self_attention(output_self_attention, pos_inputs)\n",
    "        output_feed_forward = self.feed_forward(output_layer_norm_self)\n",
    "        output_layer_norm_feed = self.layer_norm_feed_forward(output_feed_forward, output_layer_norm_self)\n",
    "        y_pred = torch.softmax(output_layer_norm_feed, dim = 0)\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self, y_pred, y_true, learning_rate = 0.03):\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optimizer = opt.SGD([self.Wq, self.Wk, self.Wv, self.Wo, \n",
    "                             self.Wnorm_self, self.bnorm_self, self.gamma_self, self.beta_self, \n",
    "                             self.Wf, self.bf, \n",
    "                             self.Wnorm_feed, self.bnorm_feed, self.gamma_feed, self.beta_feed],\n",
    "                             lr = learning_rate)\n",
    "        L = loss(y_pred, y_true)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    def process(self, data, run_backward = False):\n",
    "        items = list(data.items())\n",
    "        accuracy = 0\n",
    "        for x, y_true in items:\n",
    "            # Transform a sentence to a matrix\n",
    "            inputs = createInputs(x)\n",
    "            probs = self.forward(inputs)\n",
    "            # True label\n",
    "            true_index = int(y_true)  \n",
    "            # Accuracy\n",
    "            accuracy += int(torch.argmax(probs) == true_index) \n",
    "            if run_backward:\n",
    "                y_true_torch = torch.zeros((2,1))\n",
    "                y_true_torch[true_index] = 1\n",
    "                L = self.loss(probs, y_true_torch)\n",
    "                self.optimizer.zero_grad()\n",
    "                L.backward()\n",
    "                self.optimizer.step()\n",
    "         # Accuracy \n",
    "        return float(accuracy/len(data))\n",
    "    \n",
    "    def fit(self, data, max_iter = 201, learning_rate = 0.001):\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = opt.SGD([self.Wq, self.Wk, self.Wv, self.Wo, \n",
    "                                self.Wnorm_self, self.bnorm_self, self.gamma_self, self.beta_self, \n",
    "                                self.Wf, self.bf, \n",
    "                                self.Wnorm_feed, self.bnorm_feed, self.gamma_feed, self.beta_feed],\n",
    "                                lr = learning_rate)\n",
    "        for i in range(max_iter):\n",
    "            accuracy = self.process(data, run_backward=True)\n",
    "            if(i % 20 == 0):\n",
    "                print(f\"Step: {i}\")\n",
    "                print(f\"accuracy for training data: {accuracy}\")\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.process(data, run_backward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "accuracy for training data: 0.39655172413793105\n",
      "Step: 100\n",
      "accuracy for training data: 0.5517241379310345\n",
      "Step: 200\n",
      "accuracy for training data: 0.5517241379310345\n",
      "Step: 300\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 400\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 500\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 600\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 700\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 800\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 900\n",
      "accuracy for training data: 0.8620689655172413\n",
      "Step: 1000\n",
      "accuracy for training data: 0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "model = transformer(vocab_size, 2)\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
